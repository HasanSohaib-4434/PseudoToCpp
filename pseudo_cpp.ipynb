{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE7intZtoGvB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Dense, Dropout, LayerNormalization, Add\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "data_path = 'cleaned_pseudocode_cpp.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "tokenizer_pseudo = Tokenizer()\n",
        "tokenizer_cpp = Tokenizer()\n",
        "tokenizer_pseudo.fit_on_texts(df['text'])\n",
        "tokenizer_cpp.fit_on_texts(df['code'])\n",
        "\n",
        "pseudo_sequences = tokenizer_pseudo.texts_to_sequences(df['text'])\n",
        "cpp_sequences = tokenizer_cpp.texts_to_sequences(df['code'])\n",
        "\n",
        "max_seq_len = max(max(len(seq) for seq in pseudo_sequences), max(len(seq) for seq in cpp_sequences))\n",
        "\n",
        "pseudo_padded = pad_sequences(pseudo_sequences, maxlen=max_seq_len, padding='post')\n",
        "cpp_padded = pad_sequences(cpp_sequences, maxlen=max_seq_len + 1, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(pseudo_padded, cpp_padded, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8hOk2MNoKfv"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_len, embed_dim):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]\n",
        "    i = np.arange(embed_dim)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
        "    angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pe = tf.convert_to_tensor(angle_rads, dtype=tf.float32)\n",
        "    return tf.expand_dims(pe, axis=0)\n",
        "\n",
        "def transformer_block(inputs, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(dropout)(attn_output)\n",
        "    attn_output = Add()([inputs, attn_output])\n",
        "    attn_output = LayerNormalization(epsilon=1e-6)(attn_output)\n",
        "\n",
        "    ffn_output = Dense(ff_dim, activation=\"relu\")(attn_output)\n",
        "    ffn_output = Dense(embed_dim)(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    ffn_output = Add()([attn_output, ffn_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ffn_output)\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "def encoder(vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_layers):\n",
        "    inputs = Input(shape=(max_seq_len,))\n",
        "    embedding_layer = Embedding(vocab_size, embed_dim)(inputs)\n",
        "\n",
        "    position_encoding = positional_encoding(max_seq_len, embed_dim)\n",
        "    position_encoding_layer = Lambda(lambda x: x + position_encoding)(embedding_layer)\n",
        "\n",
        "    x = position_encoding_layer\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_block(x, embed_dim, num_heads, ff_dim)\n",
        "\n",
        "    return Model(inputs, x)\n",
        "\n",
        "def decoder(vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_layers):\n",
        "    inputs = Input(shape=(max_seq_len,))\n",
        "    embedding_layer = Embedding(vocab_size, embed_dim)(inputs)\n",
        "\n",
        "    position_encoding = positional_encoding(max_seq_len, embed_dim)\n",
        "    position_encoding_layer = Lambda(lambda x: x + position_encoding)(embedding_layer)\n",
        "\n",
        "    x = position_encoding_layer\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_block(x, embed_dim, num_heads, ff_dim)\n",
        "\n",
        "    return Model(inputs, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KnHz-1jFGzo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "input_vocab_size = len(tokenizer_pseudo.word_index) + 1\n",
        "output_vocab_size = len(tokenizer_cpp.word_index) + 1\n",
        "\n",
        "encoder_model = encoder(input_vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_layers)\n",
        "decoder_model = decoder(output_vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_layers)\n",
        "\n",
        "encoder_inputs = Input(shape=(max_seq_len,))\n",
        "decoder_inputs = Input(shape=(max_seq_len,))\n",
        "\n",
        "encoder_outputs = encoder_model(encoder_inputs)\n",
        "decoder_outputs = decoder_model(decoder_inputs)\n",
        "\n",
        "final_outputs = Dense(output_vocab_size, activation=\"softmax\")(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], final_outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "model.fit([X_train, y_train[:, :-1]], y_train[:, 1:], epochs=epochs, batch_size=batch_size, validation_data=([X_test, y_test[:, :-1]], y_test[:, 1:]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "D0IzzUPlSv0d"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('tokenizer_pseudo.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer_pseudo, f)\n",
        "\n",
        "with open('tokenizer_cpp.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer_cpp, f)\n",
        "\n",
        "model.save('transformer_model.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B0-q7ETZKjqk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open('tokenizer_pseudo.pkl', 'rb') as f:\n",
        "    tokenizer_pseudo = pickle.load(f)\n",
        "\n",
        "with open('tokenizer_cpp.pkl', 'rb') as f:\n",
        "    tokenizer_cpp = pickle.load(f)\n",
        "\n",
        "model = load_model('transformer_model.keras')\n",
        "\n",
        "def generate_code(pseudo_text, max_seq_len=100):\n",
        "    pseudo_seq = tokenizer_pseudo.texts_to_sequences([pseudo_text])\n",
        "    pseudo_padded = pad_sequences(pseudo_seq, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "    generated_tokens = [tokenizer_cpp.word_index['<start>']]\n",
        "\n",
        "    for _ in range(max_seq_len):\n",
        "        decoder_input = pad_sequences([generated_tokens], maxlen=max_seq_len, padding='post')\n",
        "        pred_probs = model.predict([pseudo_padded, decoder_input], verbose=0)\n",
        "        next_token = np.argmax(pred_probs[0, len(generated_tokens) - 1])\n",
        "\n",
        "        if next_token == tokenizer_cpp.word_index.get('<end>', 0):\n",
        "            break\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "    cpp_code = ' '.join(tokenizer_cpp.index_word.get(token, '') for token in generated_tokens if token > 0)\n",
        "    return cpp_code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoHbWfxUP7VN"
      },
      "outputs": [],
      "source": [
        "\n",
        "ui = input(\"Enter pseudocode: \")\n",
        "generated_cpp = generate_code(ui)\n",
        "print(\"Generated C++ Code:\", generated_cpp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
